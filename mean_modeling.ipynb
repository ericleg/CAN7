{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seabron style and matplotlib parameters for very nice plots\n",
    "sns.set_style('whitegrid')\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "mpl.rcParams['legend.title_fontsize'] = 14\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['figure.figsize'] = [10,6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian model for inferring the mean of a dataset\n",
    "This notebook introduces Bayesian inference with PyMC by taking the example of inferring the mean of a self-generated data set.\n",
    "## Generate some data\n",
    "First, we need some data. We can simply sample from a Normal distribution, because most things in psychology follow a Normal distribution. From these samples we want to infer the mean and the standard deviation. As we generated the data, we know the 'true' values of $\\mu$ and $\\sigma$ and can compare it with our inferred results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random number generator (rng) of NumPy\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# sample data from Normal distribution\n",
    "n = 1000 # number of data points\n",
    "muTrue = 0 # mean of the normal distribution\n",
    "sdTrue = 1 # standard deviation of the normal distribtion\n",
    "data = rng.normal(muTrue, sdTrue, n) # sample random data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa92843",
   "metadata": {},
   "source": [
    "We can plot our samples as histogram and add the empirical mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram with empirical mean and sd\n",
    "plt.hist(data, bins=30, density=True)\n",
    "plt.vlines(data.mean(), 0, 1, colors='#d28140', label=f'$M$ = {data.mean().round(2)}')\n",
    "plt.vlines([data.mean()-data.std(), data.mean()+data.std()], 0, 1, colors='#e6cdae', \n",
    "            label=f'$SD$ = {data.std().round(2)}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f13784cd",
   "metadata": {},
   "source": [
    "## Bayesian Workflow\n",
    "The process of Bayesian modeling can be divided into three steps, also sometimes called Bayesian workflow:\n",
    "1. <b>Design the model</b> based on the assumptions and how your data could have been generated.\n",
    "2. <b>Bayesian inference</b> using Bayes' theorem to get the posterior distribution.\n",
    "3. <b>Model checks</b> according to different criteria (e.g. convergence checks, expertise).\n",
    "\n",
    "The first part is the most important and hardest part. Because you have to determine the relations between your parameters and variables, select appropriate probability distributions, and set the priors. The second part is solved by our probabilistic programming language (i.e. PyMC or any other PPL). The last part is also important, because we only have an approximation of the real posterior, and the checks try to ensure that we can trust our results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the model\n",
    "One possible approach is to start with the data (i.e. dependent variable) and select the probability distribution that describes the data. Normally each probability distribution is defined by parameters (e.g. Normal: $\\mu$ and $\\sigma$) and as next step we have to define how these parameters are modeled. One possibility is to infer these parameters and select a probability distribution for each parameter again. Again these distributions have parameters, but now we can set these parameters to specific values. This would define our <font color=#8f3985><b>prior</b></font> and Bayesian inference infers a posterior for these. The other possibility would be that the parameters are defined by other variables and parameters. Here we would have to add these to our model too."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#D28140><u>Exercise:</u></font>\n",
    "How could the model for inferring the mean and standard deviation look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Bayesian models with PyMC\n",
    "Now we have to translate our designed model into a PyMC model.\n",
    "\n",
    "For model definitions in PyMC we can use a so-called <b>context manager</b>:\n",
    "\n",
    "<code><b>with</b> pm.Model() <b>as</b> modelName:\n",
    "    <i>code</i></code>\n",
    "    \n",
    "to collect all information (i.e. prior and likelihood) about our model. (Otherwise we have to link each variable to the model.)\n",
    "\n",
    "We can define a parameter and the corresponding distribution with:\n",
    "\n",
    "<code>parameter = pm.Distribution('parameterName', parameters of the distribution)</code>, e.g.\n",
    "\n",
    "<code>mu = pm.Normal('mu', mu=0, sigma=1)</code>\n",
    "\n",
    "For the <font color=#8f3985><b>Likelihood</b></font> we have to tell PyMC, that we observed data for this particular distribution, so we have to add <code>observed=data</code> after the parameter definitions, e.g. <code>pm.Normal(..., <b>observed=data</b>)</code>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "630b77f8",
   "metadata": {},
   "source": [
    "### <font color=#D28140><u>Exercise:</u></font>\n",
    "Implement the model for inferring the mean and standard deviation in PyMC. You can check yor model by just typing the name of the model in a code cell. (You can also print the model as graphical model with the graphviz package and <code>pm.model_graph.model_to_graphviz(model=modelName)</code>. You can install graphviz with <code>conda install -c conda-forge python-graphviz</code>.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Predictive Checks\n",
    "As last check before we start our sampling or inference we can look at our <font color=#8f3985><b>prior predictive distribution</b></font>. This is the distribution of the data we expect according to our defined model and priors, before actually seeing any observed data. The process of sampling from the prior predictive distribution is called <font color=#8f3985><b>prior predictive checks</b></font>.\n",
    "\n",
    "We can sample from the prior predictive distribution with <code>pm.sample_prior_predictive(model=modelName)</code> and then plot a histogram (or go for a KDE). For more complex cognitive models this step becomes really important and helpful. You can also look at the samples from priors. This can be helpful for priors that are created from other priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorPredictive = pm.sample_prior_predictive(model=meanModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1741c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "priorPredictive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac375e21",
   "metadata": {},
   "source": [
    "You can access samples with: <code>az.extract(inferenceData, group, var_names)</code>. Some extracted data has more than one dimension and you have to convert it into a vector to plot a histogram. You can do this by adding <code>to_numpy().flatten()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(az.extract(priorPredictive, group='prior', var_names='mu'), bins=100, density=True)\n",
    "plt.title(f'Prior Predictive Distribution of $\\mu$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(az.extract(priorPredictive, group='prior', var_names='sd'), bins=100, density=True)\n",
    "plt.title(f'Prior Predictive Distribution of $\\sigma$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(az.extract(priorPredictive, group='prior_predictive', var_names='Y').to_numpy().flatten(), bins=100, density=True)\n",
    "plt.title(f'Prior Predictive Distribution of $Y$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0d3ed5f",
   "metadata": {},
   "source": [
    "### <font color=#D28140><u>Exercise:</u></font>\n",
    "Modify the priors and have a look how it changes the prior predictive distributions. Guess how it will change before plotting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the inference (i.e. sample the posterior)\n",
    "After successful prior predictive checks we can start to infer the posterior. Hence, we can finally start to tell PyMC to do <font color=#8f3985><b>MCMC</b></font>-sampling to approximate our <font color=#8f3985><b>posterior</b></font> distributions. This is just one line of code <code>pm.sample()</code>! (You can also add this line after model definitions at the context manager.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceData = pm.sample(model=meanModel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11e463c5",
   "metadata": {},
   "source": [
    "As you can see, PyMC automatically selects an appropriate sampling algorithm (here NUTS), the number of chains (based on your hardware), and the number of samples. So PyMC does all the annoying stuff for us! (You can always change this default settings if you want or need to.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd822e77",
   "metadata": {},
   "source": [
    "## Inspect Posterior\n",
    "PyMC gives us an inference data (iData) object as result, which contains the posterior samples (which are most interesting), but also some statistics, and the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fe095",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceData"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f210cf",
   "metadata": {},
   "source": [
    "We can use <b>ArviZ</b> for plotting and statistics. (You find an overview here: https://python.arviz.org/en/stable/api/index.html) For instance, you can plot a KDE of the posterior (inclusive HDI) with <code>az.plot_posterior(iData, hdi_prob=HDI)</code> and get some statistics with <code>az.summary(iData)</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(inferenceData, hdi_prob=.95);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(inferenceData, hdi_prob=.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Sampling diagnostics\n",
    "The posterior distributions are approximations based on sampling and not exact calculations. Hence, before we start to interpret our posterior we have to check our sampling process. Just if we cannot find any hints that something went wrong while sampling, we can interpret the posterior with good conscience. To be able to understand the different diagnostic measures, it is important to remember how our posterior is inferred. Here is a quick reminder: we sample a markov chain that approximates the posterior distribution (i.e. with MCMC), so values with a higher posterior probability are sampled more often. Based on all samples we can create a histogram (or KDE) that shows the approximated posterior distribution. All samples are called the <b>trace</b>. While one trace can be made up of different <b>chains</b>. It is always clever to use more than one chain, because it is possible to get stuck in a local minima. As all chains have different start values we increase the probability to detect this problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6846b73",
   "metadata": {},
   "source": [
    "### <font color=#D28140><u>Exercise:</u></font>\n",
    "Go to https://bayesiancomputationbook.com/markdown/chp_02.html and choose one of the following dignostics: ESS, $\\hat R$, or MCSE. Explain the concept of the diagnostic, calculate and / or plot it, and interpret the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Effective sample size (ESS)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Potential scale reduction factor</font> $\\hat{R}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Monte Carlo standard error (MCSE)</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Trace plots</font>\n",
    "<font color=#8f3985><b>Trace plots</b></font> show the sampled values and histogarms or KDEs for all chains. So we can check if all chains converged to the same distribution and we can see the strength of autocorrelation. The function is <code>az.plot_trace(posterior)</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(inferenceData);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the distributions on the left should overlap, so all chains converged to the same distribution. And on the right we should see no trend, i.e. low autocorrelation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Autocorrelation plots</font>\n",
    "This is just another qualitative check of the autocorrelation, after we have quantified the autocorrelation with ESS. We can plot the autocorrelation for each chain and parameter with the function <code>az.plot_autocorr(posterior)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(inferenceData);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Rank plots</font>\n",
    "This is also just another qualitative tool to compare the sampling behavior of the different chains. Here all samples from the trace are ranked and then the ranks are plotted for each chain separately. So if we have no bias we should have a uniform distribution, i.e. the same amount of samples from each chain within all rank bins. Get rank plots with <code>az.plot_rank(posterior)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_rank(inferenceData);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#495F75>Divergences</font>\n",
    "<font color=#8f3985><b>Divergences</b></font> happen, when something went wrong during a sample, e.g. the sampler cannot sample a parameter value. You can get the number of divergent samples with: <code>iData.sample_stats.data_vars['diverging'].sum()</code>. If we have some divergences we can have a look at the pair plot with <code>az.plot_pair(posterior, divergences=True)</code> to (maybe) get a clue about the problem. If you have many divergences you probably should modify (i.e. <font color=#8f3985><b>reparameterize</b></font>) your model. (More about divergences: https://www.youtube.com/watch?v=6JDhM2MIYh0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenceData.sample_stats.data_vars['diverging'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(inferenceData, divergences=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks (PPC)\n",
    "As a final step we can use our posterior for sampling and can evaluate visually how close these samples from the inferred posterior match our actual data. <code>pm.sample_posterior_predictive(iData, model=modelName)</code> is the PyMC function for PPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriorPredictive = pm.sample_posterior_predictive(inferenceData, model=meanModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriorPredictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(az.extract(posteriorPredictive, group='posterior_predictive', var_names='Y').to_numpy().flatten(), label='PPC', density=True, bins=50)\n",
    "plt.hist(data, label='Y', density=True, bins=50, alpha=.5) # alpha=.7 -> transparency\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=#D28140><u>Exercises:</u></font>\n",
    "- It's important to understand the influence of data, prior, and likelihood on posterior. Hence, check how the following modifications affect your posterior approximation. Always make a guess first, and make some notes (if you want to).\n",
    "    - number of data points\n",
    "    - parameters of prior\n",
    "    - prior distribution\n",
    "    - likelihood distribution\n",
    "    - number of posterior samples\n",
    "- Run convergence checks for some of your modified models.\n",
    "\n",
    "<u>Hint:</u> You can create one cell with all the code you need or convert your notebook to a pure Python script, so you don't have to run all cells individually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>References:</u>\n",
    "- Martin, O. A., Kumar, R., & Lao, J. (2022). <i>Bayesian Modeling and Computation in Python.</i> Chapman and Hall/CRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: Mon Jun 17 2024\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.0\n",
      "IPython version      : 8.20.0\n",
      "\n",
      "seaborn   : 0.12.2\n",
      "pymc      : 5.6.1\n",
      "matplotlib: 3.8.4\n",
      "arviz     : 0.17.1\n",
      "numpy     : 1.25.2\n",
      "\n",
      "Watermark: 2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
